meta: {v: 1.0, status: draft, created: 2025-11-27}

public_repo:
  url: https://github.com/shillis13/ai_docs
  purpose: cross-platform doc access (iOS, Web, other AIs)
  relationship: ai_general/docs/ IS submodule (not copy)
  raw_url: "https://raw.githubusercontent.com/shillis13/ai_docs/main/<path>"

systems:
  cli_task_coordination:
    version: v5.0
    location: ai_comms/claude_cli/ (→ ~/.claude/coordination/)
    spec: protocol_taskCoordination_v5.0.yml
    purpose: async task delegation Desktop → CLI
    state: operational, to_execute/ → in_progress/ → completed/
    issues: [staged/ unclear, task spec undoc'd, zombie criteria undefined]

  chat_orchestrator:
    location: ~/bin/projects/chat_orchestrators/chat_orchestrator_puppeteer/
    spec: spec_ai_message_sender_v1.1.yml
    purpose: automated relay between AI chat interfaces
    state: Puppeteer-based, JSONL logging, manual/auto modes
    issues: [requires Chrome debug port, DOM selectors fragile]

  desktop_prompt_sender:
    location: ~/bin/projects/chat_orchestrators/claude_desktop_prompt_sender_v2.sh
    purpose: inject prompts into Desktop Claude via AppleScript
    state: clipboard-paste method, file-based trigger queue
    issues: [send-only (can't read), UI element dependent]

  pulse_system:
    location: ~/bin/ai/pulse_orchestrator.sh
    purpose: periodic wake-up for autonomous Desktop operation
    state: cron 10min, lock file, idle auto-release 30min, pause file
    issues: [cron location undoc'd, no metrics, pausing one-way]

  notification_system:
    location: ai_comms/claude/notifications/
    purpose: async notifications between systems and Desktop
    state: pending/processed dirs, YAML format
    issues: [no auto delivery, priority undefined, accumulation]

  codex_cli:
    location: ai_comms/codex_cli/
    purpose: task coordination for Codex CLI
    state: same pattern as Claude CLI, symlinked
    issues: [underutilized, no clear differentiation]

  cline_cli:
    location: ai_comms/cline/
    workspace: ai_cline/
    purpose: local agentic AI via Cline + llama-server
    backend: llama-server port 8081, Qwen3-Coder-30B-A3B
    wrapper: ~/bin/ai/cli/cline_cli.py
    state: operational, same task pattern as other CLIs
    config: .clinerules (workspace rules), no custom system prompt
    issues: [no daemon (same as others), checkpoint required]

  llama_server:
    location: ai_llama_cpp/
    models: ai_models/llm/
    purpose: local LLM inference backend
    port: 8081
    state: operational, SwiftBar managed
    issues: [manual model switching]

summary:
  all_systems: operational
  test_coverage: manual only
  priority_issues_per_system: 2-3

doc_index:
  architecture: [ai_comms/README.md, DESKTOP_CLI_CONTROL_ARCHITECTURE.md, cli_orchestration_v1.md, architectural_layer_model.yml]
  protocols: [protocol_taskCoordination_v5.0.yml, schema_taskFile_v1.0.yml, spec_ai_message_sender_v1.1.yml]
  quick_refs: [ai_comms/QUICK_REFERENCE.md, COORDINATION_DASHBOARD.md]

next_steps:
  pending: [doc staged/ workflow, task spec template, zombie detection (24h), automated tests]
  done: [maintenance playbook, doc consolidation]
