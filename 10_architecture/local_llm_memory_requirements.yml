---
metadata:
  title: Local LLM Memory Requirements
  version: 1.0.0
  created: 2026-01-09
  created_by: Claude Desktop + PianoMan
  status: active
  source: Empirical testing + apxml.com VRAM calculator
  
summary: |
  Activation memory dominates, not KV cache. M4 Max 64GB severely limits
  multi-agent local inference with capable (30B+) models.

reference_tools:
  vram_calculator: https://apxml.com/tools/vram-calculator
  model_specs: https://apxml.com/models/qwen3-30b-a3b

qwen3_30b_a3b_architecture:
  total_parameters: 30.5B
  active_parameters: 3.3B
  expert_parameters: 3.0B
  num_experts: 128
  active_experts: 8
  num_layers: 60
  attention_heads: 96
  kv_heads: 8
  attention_structure: GQA (12:1 ratio)
  hidden_dimension: unknown
  activation_function: SwigLU
  normalization: Layer Normalization
  position_embedding: RoPE
  native_context: 32768
  extended_context: 131072 (YaRN)

memory_breakdown_32k_context_q6:
  hardware: M4 Max 64GB
  model_quant: Q6
  kv_cache_quant: FP8
  sequence_length: 32768
  batch_size: 1
  concurrent_users: 1
  total_required: 51.65 GB
  components:
    shared_weights:
      size_gb: 2.25
      percent: 4.2
    expert_weights:
      size_gb: 22.50
      percent: 41.7
    activations:
      size_gb: 21.14
      percent: 39.2
      note: "THIS IS THE KILLER - nearly equals model weights"
    kv_cache:
      size_gb: 7.05
      percent: 13.1
    framework_overhead:
      size_gb: 0.97
      percent: 1.8

kv_cache_scaling:
  note: "KV cache alone is NOT the bottleneck most assume"
  per_1k_tokens_gb: 0.22
  formula: "~220 MB per 1K tokens for Q6/FP8 config"

practical_limits_m4_max_64gb:
  safe_memory_ceiling: 41.6 GB
  note: "Leave ~22GB for macOS, apps, headroom"
  
  feasible:
    - config: "1 agent × 16K context"
      estimated_gb: ~45
      verdict: "Comfortable"
    - config: "1 agent × 32K context"  
      estimated_gb: ~52
      verdict: "Tight but works"
      
  marginal:
    - config: "2 agents × 8K context"
      estimated_gb: ~55
      verdict: "Risky"
    - config: "2 agents × 16K context"
      estimated_gb: ~60
      verdict: "Will likely OOM"
      
  impossible:
    - config: "1 agent × 64K context"
      estimated_gb: ~75
      verdict: "OOM guaranteed"
    - config: "4 agent Cline swarm"
      estimated_gb: "100+ GB"
      verdict: "Fantasy"

benchmark_results_2026_01_09:
  model: lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-6bit
  hardware: M4 Max 64GB
  test_conditions:
    prompt_tokens: ~30
    completion_tokens: 500
    temperature: 0.7
  results:
    mlx_lm_server:
      tok_sec: 88
      notes: "Consistent across runs"
    vllm_mlx:
      tok_sec: 93
      notes: "Requires asyncio workaround for Python 3.14"
  conclusion: |
    ~6% difference single-stream. vllm-mlx batching advantage
    irrelevant when memory limits to 1-2 agents anyway.

key_insights:
  - "Activation memory (~21GB) nearly equals model weights - this is the killer"
  - "KV cache is only ~13% of total memory at 32K context"
  - "Theoretical KV-cache-only calculations underestimate by 40%+"
  - "M4 Max 64GB = single agent max for 30B models with real context"
  - "Multi-agent local inference requires smaller models (8B) or cloud APIs"

vllm_mlx_notes:
  version: 0.2.0
  python_issue: "RuntimeError with Python 3.14 asyncio.get_event_loop()"
  workaround: |
    python3 -c "import asyncio; asyncio.set_event_loop(asyncio.new_event_loop()); 
    from vllm_mlx.cli import main; import sys; 
    sys.argv = ['vllm-mlx', 'serve', 'MODEL_PATH', '--port', '8081']; main()"
  requires_model_field: true
  note: "mlx_lm.server simpler for single-agent use"
