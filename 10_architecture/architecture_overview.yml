---
metadata:
  title: Multi-AI Orchestration Architecture
  version: 1.0.0
  created: 2025-12-08
  maintainer: PianoMan
  status: active
  location: ai_general/docs/20_architecture/architecture_overview.yml
  purpose: |
    High-level system architecture. Breadth not depth.
    Use pointers (see:) to navigate to detailed documentation.

vision:
  summary: |
    Scalable multi-AI coordination infrastructure enabling autonomous task execution,
    persistent memory across sessions, and seamless context preservation.
  
  principles:
    - Brutal honesty over diplomacy
    - AIs as genuine collaborative partners
    - File-based coordination scales
    - Context is the limiting resource
    - Empirical validation over theoretical design

workspace:
  root: ~/Documents/AI/ai_root/
  see: ai_root_summary.md
  
  structure:
    ai_claude:
      purpose: Claude-specific state, memories, work logs
      see: ai_claude/README.md
      
    ai_chatgpt:
      purpose: ChatGPT configuration and exports
      
    ai_comms:
      purpose: Inter-AI coordination, CLI communication
      see: ai_comms/claude_cli/README.md
      
    ai_general:
      purpose: Shared resources, docs, todos, scripts
      children:
        docs: Documentation hierarchy (10-90 numbered)
        todos: Task backlog and staged work
        logs: System and agent logs
        scripts: Shared automation
      
    ai_memories:
      purpose: Processed chat histories and knowledge
      children:
        10_exported: Raw exports from platforms
        20_preprocessed: Cleaned/normalized
        30_converted: YAML format
        40_histories: Final indexed histories
        50_threads: Cross-chat conversation threads


ai_platforms:
  desktop_claude:
    role: Architect/Systems - strategic coordinator
    capabilities:
      - MCP servers (Desktop Commander, filesystem, Chrome, PDF)
      - Web search and fetch
      - Artifact creation
      - Memory system access
    responsibilities:
      - Research and design
      - Cross-cutting decisions
      - Documentation standards (UX/Tech Writer perspective)
      - Delegate execution to CLI agents
    see: ai_general/docs/70_instructions/claude/
    
  claude_cli:
    role: Autonomous execution workers
    agents:
      dev-lead: Development coordination, todos, code review
      librarian: Knowledge pipeline, ai_memories/
      custodian: Filesystem hygiene, versioning
      ops: Task execution from coordination system
    see: ai_general/docs/30_guides/cli_agent_operations.yml
    config: ~/.claude/agents.json
    wrapper: ~/bin/ai/cli/claude_cli.sh
    
  codex_mcp:
    role: Synchronous task execution
    use_cases:
      - Immediate results needed
      - Multi-step autonomous operations
      - Code review and validation
    timeout: 30-60 seconds
    
  chatgpt:
    name: Chatty
    role: Peer collaboration
    see: ai_chatgpt/

coordination:
  cli_task_system:
    location: ~/.claude/coordination/
    see: ai_general/docs/30_protocols/protocol_taskCoordination_latest.yml
    lifecycle:
      - broadcasts/ or direct/cli_{PID}/ (posted)
      - to_execute/ (ready for execution)
      - in_progress/{task}/ (claimed)
      - completed/{task}/ (done)
      - responses/cli_{PID}/ (results)
      
  pulse_automation:
    purpose: Cron-triggered autonomous work
    interval: 5-10 minutes when inactive
    status: Planned - not yet implemented
    
  chat_orchestrator:
    purpose: AI-to-AI conversation automation
    status: In development - no formal spec yet


memory_system:
  native_memory:
    slots: 30 slots Ã— 200 chars
    format: Memory pointers to external files
    see: ai_general/docs/40_specs/spec_memory_pointer_schema_v1.0.yml
    
  knowledge_pipeline:
    stages:
      - Export from platforms
      - Preprocess and normalize  
      - Convert to YAML
      - Index in histories
      - Generate summaries (colocated)
      - Build cross-chat threads
    see: ai_general/docs/40_specs/memory_pipeline_spec.yml
    
  context_preservation:
    limit: 200K tokens
    strategy: |
      - Memory pointers save 40-55K vs auto-loaded content
      - Delegate exploration to CLI/Codex
      - Monitor at 60%, handoff before 90%
    see: ai_general/docs/70_instructions/claude/context_preservation.yml

key_constraints:
  context_window: 200K tokens - fundamental architectural constraint
  mcp_timeout: 30-60 seconds hard limit
  bash_tool: Sandbox only - no filesystem access, never use
  browser_snapshots: Write to file, delegate analysis (20-50K token explosion)

tooling:
  automation:
    location: ~/bin/ai/
    categories:
      cli/: Claude CLI wrapper and session management
      utils/: Utility scripts (repo_status.py, etc.)
      
  shell_integration:
    bash_prompt: ~/.bash_prompt (modular segments)
    repo_status: ~/.repo_status (cron-updated)
    
  documentation:
    numbering: 10-90 prefix system
    format: YAML-as-source where possible
    versioning: name_vN.M.ext with *_latest symlinks

related_docs:
  specs:
    - ai_general/docs/40_specs/cli_coordination_protocol_v4.md
    - ai_general/docs/40_specs/cli_specialized_agent_roles_v2.md
    - ai_general/docs/40_specs/spec_memory_pointer_schema_v1.0.yml
    - ai_general/docs/40_specs/spec_response_footer_v1.1.yml
    
  guides:
    - ai_general/docs/30_guides/cli_agent_operations.yml
    
  instructions:
    - ai_general/docs/70_instructions/claude/
    - ai_general/docs/70_instructions/chatgpt/
